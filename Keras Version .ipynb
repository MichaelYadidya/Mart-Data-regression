{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "from plotly.offline import init_notebook_mode,download_plotlyjs,iplot\n",
    "\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "\n",
    "df_train = pd.read_csv('Train.csv')\n",
    "df_test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic analysis and features engineering\n",
    "\n",
    "#1. Removing Unwanted cloumns and features\n",
    "try:\n",
    "    df_train.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "    df_test.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of dataset is \t 8523\n"
     ]
    }
   ],
   "source": [
    "#2. Getting information about null values\n",
    "\n",
    "temp_df = df_train.isnull().sum().reset_index()\n",
    "temp_df['Percentage'] = (temp_df[0]/len(df_train))*100\n",
    "\n",
    "temp_df.columns = ['Column Name','Number of null values', 'Null values in percentage']\n",
    "\n",
    "print(f'The length of dataset is \\t {len(df_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Unique values in this column in Train set are \t ['Low Fat' 'Regular']\n",
      "Now unique values in this cloumn in test set are \t ['Low Fat' 'Regular']\n"
     ]
    }
   ],
   "source": [
    "#3. Making correction in 'Item_Fat_content' column\n",
    "\n",
    "def convert(x):\n",
    "    if x in ['low fat','LF']:\n",
    "        return 'Low Fat'\n",
    "    elif x=='reg':\n",
    "        return 'Regular'\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\n",
    "\n",
    "df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].apply(convert)\n",
    "\n",
    "print(f'Now Unique values in this column in Train set are \\t {df_train[\"Item_Fat_Content\"].unique()}')\n",
    "print(f'Now unique values in this cloumn in test set are \\t {df_test[\"Item_Fat_Content\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "marker": {
          "color": "rgba(0, 191, 255, 0.6)",
          "line": {
           "color": "rgba(0, 191, 255, 1.0)",
           "width": 1
          }
         },
         "name": "Outlet_Size",
         "orientation": "v",
         "text": "",
         "type": "bar",
         "uid": "94236972-1d7c-4d59-888b-b13c664b5b6e",
         "x": [
          "Medium",
          "Small",
          "High"
         ],
         "y": [
          2793,
          2388,
          932
         ]
        }
       ],
       "layout": {
        "legend": {
         "bgcolor": "#F5F6F9",
         "font": {
          "color": "#4D5663"
         }
        },
        "paper_bgcolor": "#F5F6F9",
        "plot_bgcolor": "#F5F6F9",
        "title": {
         "font": {
          "color": "#4D5663"
         },
         "text": "High VS Medium VS Small"
        },
        "xaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": "Size"
         },
         "zerolinecolor": "#E1E5ED"
        },
        "yaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": "Frequency"
         },
         "zerolinecolor": "#E1E5ED"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"a86d9fab-87ee-41ed-beaa-8f23d14c1c41\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"a86d9fab-87ee-41ed-beaa-8f23d14c1c41\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'a86d9fab-87ee-41ed-beaa-8f23d14c1c41',\n",
       "                        [{\"marker\": {\"color\": \"rgba(0, 191, 255, 0.6)\", \"line\": {\"color\": \"rgba(0, 191, 255, 1.0)\", \"width\": 1}}, \"name\": \"Outlet_Size\", \"orientation\": \"v\", \"text\": \"\", \"type\": \"bar\", \"uid\": \"94236972-1d7c-4d59-888b-b13c664b5b6e\", \"x\": [\"Medium\", \"Small\", \"High\"], \"y\": [2793, 2388, 932]}],\n",
       "                        {\"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}, \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"High VS Medium VS Small\"}, \"xaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Size\"}, \"zerolinecolor\": \"#E1E5ED\"}, \"yaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Frequency\"}, \"zerolinecolor\": \"#E1E5ED\"}},\n",
       "                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a86d9fab-87ee-41ed-beaa-8f23d14c1c41');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#4 Dealing with missing values in categorical type column ie 'Outlet_size'\n",
    "\n",
    "count = df_train['Outlet_Size'].value_counts().reset_index()\n",
    "count.iplot(kind='bar',color='deepskyblue',x='index',y='Outlet_Size',\n",
    "            title='High VS Medium VS Small',xTitle='Size',yTitle='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing missing values from medium in both training and test\n",
    "\n",
    "df_train['Outlet_Size'].fillna(value='Medium',inplace=True)\n",
    "df_test['Outlet_Size'].fillna(value='Medium',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating feature matrix and target vector\n",
    "\n",
    "x_train = df_train.iloc[:,:-1].values\n",
    "y_train = df_train.iloc[:,-1].values\n",
    "\n",
    "x_test = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\conda\\conda\\envs\\machinelearning\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning:\n",
      "\n",
      "Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer()\n",
    "x_train[:,[0]] = imputer.fit_transform(x_train[:,[0]])\n",
    "x_test[:,[0]] = imputer.fit_transform(x_test[:,[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\conda\\conda\\envs\\machinelearning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning:\n",
      "\n",
      "The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "\n",
      "c:\\users\\micha\\appdata\\local\\conda\\conda\\envs\\machinelearning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning:\n",
      "\n",
      "The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Dealing with categorical values in features /cloumns \n",
    "\n",
    "labelencoder_x = LabelEncoder()\n",
    "x_train[:,1] = labelencoder_x.fit_transform(x_train[:,1])\n",
    "x_train[:,3] = labelencoder_x.fit_transform(x_train[:,3])\n",
    "x_train[:,5] = labelencoder_x.fit_transform(x_train[:,5])\n",
    "x_train[:,6] = labelencoder_x.fit_transform(x_train[:,6])\n",
    "x_train[:,7] = labelencoder_x.fit_transform(x_train[:,7])\n",
    "\n",
    "onehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7])\n",
    "x_train = onehotencoder_x.fit_transform(x_train).toarray()\n",
    "\n",
    "x_test[:,1] = labelencoder_x.fit_transform(x_test[:,1])\n",
    "x_test[:,3] = labelencoder_x.fit_transform(x_test[:,3])\n",
    "x_test[:,5] = labelencoder_x.fit_transform(x_test[:,5])\n",
    "x_test[:,6] = labelencoder_x.fit_transform(x_test[:,6])\n",
    "x_test[:,7] = labelencoder_x.fit_transform(x_test[:,7])\n",
    "\n",
    "\n",
    "#need to be done when we have more than twocategorical values\n",
    "\n",
    "onehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7])\n",
    "x_test = onehotencoder_x.fit_transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply Feature scaling on feature matrix\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "x_test = sc_X.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28685487, -0.17419336, -0.11434598, ..., -0.73814723,\n",
       "        -0.97073217,  1.74745381],\n",
       "       [-0.28685487, -0.17419336, -0.11434598, ...,  1.35474328,\n",
       "        -0.90811123, -1.48902325],\n",
       "       [-0.28685487, -0.17419336, -0.11434598, ..., -0.73814723,\n",
       "        -0.95691733,  0.01004021],\n",
       "       ...,\n",
       "       [-0.28685487, -0.17419336, -0.11434598, ..., -0.73814723,\n",
       "        -0.59978449, -0.89720755],\n",
       "       [-0.28685487, -0.17419336, -0.11434598, ...,  1.35474328,\n",
       "         1.53287976, -0.60797692],\n",
       "       [-0.28685487, -0.17419336, -0.11434598, ..., -0.73814723,\n",
       "        -0.41193591, -1.05226104]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20,input_dim=30,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(1,kernel_initializer='normal'))\n",
    "model.compile(loss='mean_absolute_percentage_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6818 samples, validate on 1705 samples\n",
      "Epoch 1/200\n",
      "6818/6818 [==============================] - 1s 178us/step - loss: 54.9901 - val_loss: 53.8421\n",
      "Epoch 2/200\n",
      "6818/6818 [==============================] - 1s 121us/step - loss: 54.6859 - val_loss: 53.5704\n",
      "Epoch 3/200\n",
      "6818/6818 [==============================] - 1s 152us/step - loss: 54.3979 - val_loss: 53.3452\n",
      "Epoch 4/200\n",
      "6818/6818 [==============================] - 1s 149us/step - loss: 54.1215 - val_loss: 53.1059\n",
      "Epoch 5/200\n",
      "6818/6818 [==============================] - 1s 138us/step - loss: 53.8550 - val_loss: 52.8983\n",
      "Epoch 6/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 53.5947 - val_loss: 52.6710\n",
      "Epoch 7/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 53.3473 - val_loss: 52.4318\n",
      "Epoch 8/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 53.1153 - val_loss: 52.2020\n",
      "Epoch 9/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 52.8744 - val_loss: 51.9786\n",
      "Epoch 10/200\n",
      "6818/6818 [==============================] - 1s 126us/step - loss: 52.6414 - val_loss: 51.7911\n",
      "Epoch 11/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 52.4096 - val_loss: 51.5771\n",
      "Epoch 12/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 52.1776 - val_loss: 51.3736\n",
      "Epoch 13/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 51.9570 - val_loss: 51.2034\n",
      "Epoch 14/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 51.7386 - val_loss: 50.9951\n",
      "Epoch 15/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 51.5280 - val_loss: 50.8179\n",
      "Epoch 16/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 51.3223 - val_loss: 50.6241\n",
      "Epoch 17/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 51.1214 - val_loss: 50.4169\n",
      "Epoch 18/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 50.9171 - val_loss: 50.2416\n",
      "Epoch 19/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 50.7189 - val_loss: 50.0264\n",
      "Epoch 20/200\n",
      "6818/6818 [==============================] - 1s 124us/step - loss: 50.5239 - val_loss: 49.8409\n",
      "Epoch 21/200\n",
      "6818/6818 [==============================] - 1s 121us/step - loss: 50.3370 - val_loss: 49.6719\n",
      "Epoch 22/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 50.1538 - val_loss: 49.4865\n",
      "Epoch 23/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 49.9610 - val_loss: 49.3017\n",
      "Epoch 24/200\n",
      "6818/6818 [==============================] - 1s 121us/step - loss: 49.7831 - val_loss: 49.1357\n",
      "Epoch 25/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 49.6142 - val_loss: 48.9670\n",
      "Epoch 26/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 49.4343 - val_loss: 48.8183\n",
      "Epoch 27/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 49.2619 - val_loss: 48.6629\n",
      "Epoch 28/200\n",
      "6818/6818 [==============================] - 1s 122us/step - loss: 49.1024 - val_loss: 48.5139\n",
      "Epoch 29/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 48.9409 - val_loss: 48.3600\n",
      "Epoch 30/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 48.7888 - val_loss: 48.2409\n",
      "Epoch 31/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 48.6430 - val_loss: 48.0938\n",
      "Epoch 32/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 48.5034 - val_loss: 47.9788\n",
      "Epoch 33/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 48.3656 - val_loss: 47.8584\n",
      "Epoch 34/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 48.2378 - val_loss: 47.7481\n",
      "Epoch 35/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 48.1153 - val_loss: 47.6354\n",
      "Epoch 36/200\n",
      "6818/6818 [==============================] - 1s 127us/step - loss: 47.9976 - val_loss: 47.5085\n",
      "Epoch 37/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 47.8787 - val_loss: 47.4315\n",
      "Epoch 38/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 47.7733 - val_loss: 47.3051\n",
      "Epoch 39/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 47.6747 - val_loss: 47.1901\n",
      "Epoch 40/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 47.5797 - val_loss: 47.1036\n",
      "Epoch 41/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 47.4862 - val_loss: 47.0124\n",
      "Epoch 42/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 47.3948 - val_loss: 46.9164\n",
      "Epoch 43/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 47.3097 - val_loss: 46.8251\n",
      "Epoch 44/200\n",
      "6818/6818 [==============================] - 1s 126us/step - loss: 47.2334 - val_loss: 46.7598\n",
      "Epoch 45/200\n",
      "6818/6818 [==============================] - 1s 147us/step - loss: 47.1543 - val_loss: 46.6779\n",
      "Epoch 46/200\n",
      "6818/6818 [==============================] - 1s 136us/step - loss: 47.0832 - val_loss: 46.6008\n",
      "Epoch 47/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 47.0117 - val_loss: 46.5331\n",
      "Epoch 48/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 46.9439 - val_loss: 46.4896\n",
      "Epoch 49/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 46.8772 - val_loss: 46.4049\n",
      "Epoch 50/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 46.8143 - val_loss: 46.3288\n",
      "Epoch 51/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 46.7510 - val_loss: 46.2706\n",
      "Epoch 52/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 46.6926 - val_loss: 46.1911\n",
      "Epoch 53/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 46.6356 - val_loss: 46.1339\n",
      "Epoch 54/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 46.5775 - val_loss: 46.0909\n",
      "Epoch 55/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 46.5245 - val_loss: 46.0456\n",
      "Epoch 56/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 46.4762 - val_loss: 45.9694\n",
      "Epoch 57/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 46.4259 - val_loss: 45.9003\n",
      "Epoch 58/200\n",
      "6818/6818 [==============================] - 1s 113us/step - loss: 46.3836 - val_loss: 45.8669\n",
      "Epoch 59/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 46.3325 - val_loss: 45.8233\n",
      "Epoch 60/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 46.2913 - val_loss: 45.7997\n",
      "Epoch 61/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 46.2508 - val_loss: 45.7442\n",
      "Epoch 62/200\n",
      "6818/6818 [==============================] - 1s 138us/step - loss: 46.2095 - val_loss: 45.7058\n",
      "Epoch 63/200\n",
      "6818/6818 [==============================] - 1s 124us/step - loss: 46.1682 - val_loss: 45.6819\n",
      "Epoch 64/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 46.1374 - val_loss: 45.6361\n",
      "Epoch 65/200\n",
      "6818/6818 [==============================] - 1s 143us/step - loss: 46.0885 - val_loss: 45.5941\n",
      "Epoch 66/200\n",
      "6818/6818 [==============================] - 1s 131us/step - loss: 46.0632 - val_loss: 45.5873\n",
      "Epoch 67/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 46.0258 - val_loss: 45.5624\n",
      "Epoch 68/200\n",
      "6818/6818 [==============================] - 1s 126us/step - loss: 45.9942 - val_loss: 45.5347\n",
      "Epoch 69/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.9638 - val_loss: 45.5268\n",
      "Epoch 70/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.9317 - val_loss: 45.5030\n",
      "Epoch 71/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.9012 - val_loss: 45.4795\n",
      "Epoch 72/200\n",
      "6818/6818 [==============================] - 1s 128us/step - loss: 45.8729 - val_loss: 45.4384\n",
      "Epoch 73/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.8469 - val_loss: 45.4148\n",
      "Epoch 74/200\n",
      "6818/6818 [==============================] - 1s 114us/step - loss: 45.8144 - val_loss: 45.3915\n",
      "Epoch 75/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 45.7875 - val_loss: 45.3879\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.7592 - val_loss: 45.3510\n",
      "Epoch 77/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.7356 - val_loss: 45.3360\n",
      "Epoch 78/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 45.7121 - val_loss: 45.3083\n",
      "Epoch 79/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.6954 - val_loss: 45.3140\n",
      "Epoch 80/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 45.6727 - val_loss: 45.2674\n",
      "Epoch 81/200\n",
      "6818/6818 [==============================] - 1s 125us/step - loss: 45.6561 - val_loss: 45.2684\n",
      "Epoch 82/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 45.6307 - val_loss: 45.2487\n",
      "Epoch 83/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 45.6148 - val_loss: 45.2541\n",
      "Epoch 84/200\n",
      "6818/6818 [==============================] - 1s 124us/step - loss: 45.5937 - val_loss: 45.2207\n",
      "Epoch 85/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 45.5772 - val_loss: 45.2032\n",
      "Epoch 86/200\n",
      "6818/6818 [==============================] - 1s 128us/step - loss: 45.5533 - val_loss: 45.1963\n",
      "Epoch 87/200\n",
      "6818/6818 [==============================] - 1s 125us/step - loss: 45.5387 - val_loss: 45.1747\n",
      "Epoch 88/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 45.5240 - val_loss: 45.1498\n",
      "Epoch 89/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.5079 - val_loss: 45.1381\n",
      "Epoch 90/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.4910 - val_loss: 45.1169\n",
      "Epoch 91/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 45.4777 - val_loss: 45.1070\n",
      "Epoch 92/200\n",
      "6818/6818 [==============================] - 1s 133us/step - loss: 45.4608 - val_loss: 45.1065\n",
      "Epoch 93/200\n",
      "6818/6818 [==============================] - 1s 126us/step - loss: 45.4423 - val_loss: 45.0965\n",
      "Epoch 94/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 45.4292 - val_loss: 45.0774\n",
      "Epoch 95/200\n",
      "6818/6818 [==============================] - 1s 121us/step - loss: 45.4116 - val_loss: 45.0684\n",
      "Epoch 96/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 45.3983 - val_loss: 45.0523\n",
      "Epoch 97/200\n",
      "6818/6818 [==============================] - 1s 136us/step - loss: 45.3803 - val_loss: 45.0511\n",
      "Epoch 98/200\n",
      "6818/6818 [==============================] - 1s 132us/step - loss: 45.3760 - val_loss: 45.0539\n",
      "Epoch 99/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 45.3513 - val_loss: 45.0355\n",
      "Epoch 100/200\n",
      "6818/6818 [==============================] - 1s 127us/step - loss: 45.3394 - val_loss: 45.0096\n",
      "Epoch 101/200\n",
      "6818/6818 [==============================] - 1s 125us/step - loss: 45.3220 - val_loss: 45.0078\n",
      "Epoch 102/200\n",
      "6818/6818 [==============================] - 1s 131us/step - loss: 45.3123 - val_loss: 45.0100\n",
      "Epoch 103/200\n",
      "6818/6818 [==============================] - 1s 122us/step - loss: 45.2925 - val_loss: 45.0180\n",
      "Epoch 104/200\n",
      "6818/6818 [==============================] - 1s 125us/step - loss: 45.2741 - val_loss: 44.9766\n",
      "Epoch 105/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 45.2613 - val_loss: 45.0108\n",
      "Epoch 106/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 45.2492 - val_loss: 44.9870\n",
      "Epoch 107/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 45.2339 - val_loss: 45.0067\n",
      "Epoch 108/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.2177 - val_loss: 44.9653\n",
      "Epoch 109/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.2113 - val_loss: 44.9737\n",
      "Epoch 110/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 45.1934 - val_loss: 44.9689\n",
      "Epoch 111/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.1818 - val_loss: 44.9918\n",
      "Epoch 112/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 45.1689 - val_loss: 44.9670\n",
      "Epoch 113/200\n",
      "6818/6818 [==============================] - 1s 131us/step - loss: 45.1538 - val_loss: 44.9893\n",
      "Epoch 114/200\n",
      "6818/6818 [==============================] - 1s 125us/step - loss: 45.1401 - val_loss: 44.9477\n",
      "Epoch 115/200\n",
      "6818/6818 [==============================] - 1s 147us/step - loss: 45.1299 - val_loss: 44.9630\n",
      "Epoch 116/200\n",
      "6818/6818 [==============================] - 1s 166us/step - loss: 45.1211 - val_loss: 44.9660\n",
      "Epoch 117/200\n",
      "6818/6818 [==============================] - 1s 146us/step - loss: 45.1092 - val_loss: 44.9431\n",
      "Epoch 118/200\n",
      "6818/6818 [==============================] - 1s 152us/step - loss: 45.0945 - val_loss: 44.9290\n",
      "Epoch 119/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 45.0838 - val_loss: 44.9319\n",
      "Epoch 120/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 45.0709 - val_loss: 44.9243\n",
      "Epoch 121/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 45.0635 - val_loss: 44.9357\n",
      "Epoch 122/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 45.0467 - val_loss: 44.9355\n",
      "Epoch 123/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 45.0413 - val_loss: 44.9450\n",
      "Epoch 124/200\n",
      "6818/6818 [==============================] - 1s 125us/step - loss: 45.0300 - val_loss: 44.9262\n",
      "Epoch 125/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 45.0216 - val_loss: 44.9099\n",
      "Epoch 126/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 45.0116 - val_loss: 44.9008\n",
      "Epoch 127/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.9956 - val_loss: 44.8970\n",
      "Epoch 128/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 44.9876 - val_loss: 44.8984\n",
      "Epoch 129/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.9795 - val_loss: 44.9138\n",
      "Epoch 130/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.9740 - val_loss: 44.9278\n",
      "Epoch 131/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 44.9620 - val_loss: 44.9157\n",
      "Epoch 132/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 44.9514 - val_loss: 44.9131\n",
      "Epoch 133/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.9479 - val_loss: 44.9054\n",
      "Epoch 134/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 44.9335 - val_loss: 44.9057\n",
      "Epoch 135/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 44.9301 - val_loss: 44.8996\n",
      "Epoch 136/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.9175 - val_loss: 44.8888\n",
      "Epoch 137/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.9084 - val_loss: 44.8931\n",
      "Epoch 138/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.8988 - val_loss: 44.9009\n",
      "Epoch 139/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.8973 - val_loss: 44.9001\n",
      "Epoch 140/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.8816 - val_loss: 44.8972\n",
      "Epoch 141/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 44.8751 - val_loss: 44.8806\n",
      "Epoch 142/200\n",
      "6818/6818 [==============================] - 1s 124us/step - loss: 44.8673 - val_loss: 44.9085\n",
      "Epoch 143/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.8544 - val_loss: 44.8907\n",
      "Epoch 144/200\n",
      "6818/6818 [==============================] - 1s 126us/step - loss: 44.8590 - val_loss: 44.8726\n",
      "Epoch 145/200\n",
      "6818/6818 [==============================] - 1s 122us/step - loss: 44.8418 - val_loss: 44.8839\n",
      "Epoch 146/200\n",
      "6818/6818 [==============================] - 1s 115us/step - loss: 44.8370 - val_loss: 44.8991\n",
      "Epoch 147/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.8288 - val_loss: 44.8673\n",
      "Epoch 148/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.8242 - val_loss: 44.8528\n",
      "Epoch 149/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.8119 - val_loss: 44.8607\n",
      "Epoch 150/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 44.8066 - val_loss: 44.8601\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6818/6818 [==============================] - 1s 116us/step - loss: 44.7929 - val_loss: 44.8619\n",
      "Epoch 152/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 44.7852 - val_loss: 44.8684\n",
      "Epoch 153/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.7832 - val_loss: 44.8583\n",
      "Epoch 154/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.7654 - val_loss: 44.8545\n",
      "Epoch 155/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.7623 - val_loss: 44.8513\n",
      "Epoch 156/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 44.7535 - val_loss: 44.8598\n",
      "Epoch 157/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.7425 - val_loss: 44.8361\n",
      "Epoch 158/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 44.7376 - val_loss: 44.8383\n",
      "Epoch 159/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.7314 - val_loss: 44.8310\n",
      "Epoch 160/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.7250 - val_loss: 44.8400\n",
      "Epoch 161/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.7163 - val_loss: 44.8283\n",
      "Epoch 162/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 44.7060 - val_loss: 44.8449\n",
      "Epoch 163/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 44.6987 - val_loss: 44.8306\n",
      "Epoch 164/200\n",
      "6818/6818 [==============================] - 1s 119us/step - loss: 44.6945 - val_loss: 44.8161\n",
      "Epoch 165/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.6809 - val_loss: 44.8012\n",
      "Epoch 166/200\n",
      "6818/6818 [==============================] - 1s 118us/step - loss: 44.6765 - val_loss: 44.8056\n",
      "Epoch 167/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.6656 - val_loss: 44.7970\n",
      "Epoch 168/200\n",
      "6818/6818 [==============================] - 1s 116us/step - loss: 44.6669 - val_loss: 44.7973\n",
      "Epoch 169/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.6555 - val_loss: 44.7803\n",
      "Epoch 170/200\n",
      "6818/6818 [==============================] - 1s 117us/step - loss: 44.6433 - val_loss: 44.8104\n",
      "Epoch 171/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 44.6363 - val_loss: 44.7803\n",
      "Epoch 172/200\n",
      "6818/6818 [==============================] - 1s 139us/step - loss: 44.6296 - val_loss: 44.7775\n",
      "Epoch 173/200\n",
      "6818/6818 [==============================] - 1s 132us/step - loss: 44.6239 - val_loss: 44.7697\n",
      "Epoch 174/200\n",
      "6818/6818 [==============================] - 1s 144us/step - loss: 44.6081 - val_loss: 44.7559\n",
      "Epoch 175/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 44.6023 - val_loss: 44.7360\n",
      "Epoch 176/200\n",
      "6818/6818 [==============================] - 1s 130us/step - loss: 44.5978 - val_loss: 44.7534\n",
      "Epoch 177/200\n",
      "6818/6818 [==============================] - 1s 129us/step - loss: 44.5853 - val_loss: 44.7420\n",
      "Epoch 178/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 44.5822 - val_loss: 44.7467\n",
      "Epoch 179/200\n",
      "6818/6818 [==============================] - 1s 120us/step - loss: 44.5700 - val_loss: 44.7380\n",
      "Epoch 180/200\n",
      "6818/6818 [==============================] - 1s 169us/step - loss: 44.5636 - val_loss: 44.7186\n",
      "Epoch 181/200\n",
      "6818/6818 [==============================] - 1s 179us/step - loss: 44.5556 - val_loss: 44.7339\n",
      "Epoch 182/200\n",
      "6818/6818 [==============================] - 1s 173us/step - loss: 44.5467 - val_loss: 44.7218\n",
      "Epoch 183/200\n",
      "6818/6818 [==============================] - 1s 147us/step - loss: 44.5485 - val_loss: 44.7162\n",
      "Epoch 184/200\n",
      "6818/6818 [==============================] - 1s 134us/step - loss: 44.5387 - val_loss: 44.7294\n",
      "Epoch 185/200\n",
      "6818/6818 [==============================] - 1s 131us/step - loss: 44.5301 - val_loss: 44.7355\n",
      "Epoch 186/200\n",
      "6818/6818 [==============================] - 1s 130us/step - loss: 44.5246 - val_loss: 44.7299\n",
      "Epoch 187/200\n",
      "6818/6818 [==============================] - 1s 158us/step - loss: 44.5182 - val_loss: 44.7248\n",
      "Epoch 188/200\n",
      "6818/6818 [==============================] - ETA: 0s - loss: 44.54 - 1s 161us/step - loss: 44.5136 - val_loss: 44.7427\n",
      "Epoch 189/200\n",
      "6818/6818 [==============================] - 1s 123us/step - loss: 44.5104 - val_loss: 44.7439\n",
      "Epoch 190/200\n",
      "6818/6818 [==============================] - 1s 122us/step - loss: 44.5013 - val_loss: 44.7233\n",
      "Epoch 191/200\n",
      "6818/6818 [==============================] - 1s 122us/step - loss: 44.4946 - val_loss: 44.7215\n",
      "Epoch 192/200\n",
      "6818/6818 [==============================] - 1s 121us/step - loss: 44.4863 - val_loss: 44.7291\n",
      "Epoch 193/200\n",
      "6818/6818 [==============================] - 1s 131us/step - loss: 44.4908 - val_loss: 44.7201\n",
      "Epoch 194/200\n",
      "6818/6818 [==============================] - 1s 152us/step - loss: 44.4775 - val_loss: 44.7102\n",
      "Epoch 195/200\n",
      "6818/6818 [==============================] - 1s 136us/step - loss: 44.4737 - val_loss: 44.7175\n",
      "Epoch 196/200\n",
      "6818/6818 [==============================] - 1s 130us/step - loss: 44.4638 - val_loss: 44.7184\n",
      "Epoch 197/200\n",
      "6818/6818 [==============================] - 1s 146us/step - loss: 44.4614 - val_loss: 44.7154\n",
      "Epoch 198/200\n",
      "6818/6818 [==============================] - 1s 151us/step - loss: 44.4560 - val_loss: 44.7024\n",
      "Epoch 199/200\n",
      "6818/6818 [==============================] - 1s 145us/step - loss: 44.4469 - val_loss: 44.7286\n",
      "Epoch 200/200\n",
      "6818/6818 [==============================] - 1s 145us/step - loss: 44.4437 - val_loss: 44.7016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22511426b38>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_split=0.2,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1173.8251],\n",
       "       [1012.3114],\n",
       "       [ 433.6255],\n",
       "       [1784.8308],\n",
       "       [3496.5862]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
